<events> 
    <talk>
        <speaker>Andrew Davison</speaker>
        <affliation>Imperial College London</affliation>
        <date>15 May 2020</date>
        <title>From SLAM to Spatial AI</title>
        <abstract>To enable the next generation of smart robots and devices which can truly interact with their environments, Simultaneous Localisation and Mapping (SLAM) will progressively develop into a general real-time geometric and semantic `Spatial AI' perception capability. I will give many examples from our work on gradually increasing visual SLAM capability over the years. However, much research must still be done to achieve true Spatial AI performance. A key issue is how estimation and machine learning components can be used and trained together as we continue to search for the best long-term scene representations to enable intelligent interaction.  Further, to enable the performance and efficiency required by real products, computer vision algorithms must be developed together with the sensors and processors which form full systems, and I will cover research on vision algorithms for non-standard visual sensors and graph-based computing architectures.</abstract>
        <bio>Andrew Davison is Professor of Robot Vision and Director of the Dyson Robotics Laboratory at Imperial College London. His long-term research focus is on SLAM (Simultaneous Localisation and Mapping) and its evolution towards general `Spatial AI': computer vision algorithms which enable robots and other artificial devices to map, localise within and ultimately understand and interact with the 3D spaces around them. With his research group and collaborators he has consistently developed and demonstrated breakthrough systems, including MonoSLAM, KinectFusion, SLAM++ and CodeSLAM, and recent prizes include Best Paper at ECCV 2016 and Best Paper Honourable Mention at CVPR 2018. He has also had strong involvement in taking this technology into real applications, in particular through his work with Dyson on the design of the visual mapping system inside the Dyson 360 Eye robot vacuum cleaner and as co-founder of applied SLAM start-up SLAMcore. He was elected Fellow of the Royal Academy of Engineering in 2017.</bio>
        <graphic>andrew_davison.jpg</graphic>
        <video>https://www.youtube.com/embed/lGIM2WVp5t0?rel0&amp;controls=0&amp;showinfo=0</video>
        <url>https://www.doc.ic.ac.uk/~ajd/</url>
        <guest>John Leonard</guest>
        <guesturl>http://marinerobotics.mit.edu/john-j-leonard</guesturl>
        <qa>andrew_davison.xml</qa>
    </talk>
    <talk>
        <speaker>Leslie Kaelbling</speaker>
        <affliation>MIT</affliation>
        <date>22 May 2020</date>
        <title>Doing for our robots what nature did for us</title>
        <abstract>We, as robot engineers, have to think hard about our role in the design of robots and how it interacts with learning, both in "the factory" (that is, at engineering time) and in "the wild" (that is, when the robot is delivered to a customer). I will share some general thoughts about the strategies for robot design and then talk in detail about some work I have been involved in, both in the design of an overall architecture for an intelligent robot and in strategies for learning to integrate new skills into the repertoire of an already competent robot.</abstract>
        <bio>Leslie is a Professor at MIT. She has an undergraduate degree in Philosophy and a PhD in Computer Science from Stanford, and was previously on the faculty at Brown University. She was the founding editor-in-chief of the Journal of Machine Learning Research. Her research agenda is to make intelligent robots using methods including estimation, learning, planning, and reasoning. She is not a robot.</bio>
        <graphic>leslie_kaelbling.jpg</graphic>
        <video>https://www.youtube.com/embed/5R-xL9YmdR0?rel0&amp;controls=0&amp;showinfo=0</video>
        <url>https://people.csail.mit.edu/lpk/</url>
    </talk>
    <talk>
        <speaker>Allison Okamura</speaker>
        <affliation>Stanford</affliation>
        <date>5 June 2020</date>
        <title>Soft Robots for Humanity</title>
        <abstract>Traditional robotic manipulators are constructed from rigid links and localized joints – this enables large forces and workspaces, but restricts access and creates safety challenges. In contrast, many soft robots have a volumetric form factor and continuous bending that takes advantage of their flexible, deformable materials to access difficult spaces, but these same mechanical properties can hinder manipulation. This talk will examine robotic systems that achieve the best of both worlds by leveraging both softness and rigidity for novel shape control, a compliant interface to the human body, and accessing hard-to-reach locations. Knowing when to exploit and when to alter some of the inherent consequences of softness is key to making soft robots that can be deployed to benefit human health, safety, and quality of life.</abstract>
        <bio>Allison M. Okamura received the BS degree from the University of California at Berkeley and the MS and PhD degrees from Stanford University, all in mechanical engineering. She is currently Professor in the mechanical engineering department at Stanford University, with a courtesy appointment in computer science. She is an IEEE Fellow and Editor-in-Chief of the journal IEEE Robotics and Automation Letters. Her awards include the 2019 IEEE Robotics and Automation Society Distinguished Service Award, 2016 Duca Family University Fellow in Undergraduate Education, 2009 IEEE Technical Committee on Haptics Early Career Award, 2005 IEEE Robotics and Automation Society Early Academic Career Award, and 2004 NSF CAREER Award. Her academic interests include haptics, teleoperation, virtual environments and simulators, medical robotics, neuromechanics and rehabilitation, prosthetics, and engineering education. Outside academia, she enjoys spending time with her husband and two children, running, and playing ice hockey.</bio>
        <graphic>allison_okamura.jpg</graphic>
        <video>https://www.youtube.com/embed/N6_jmD_89qM?rel0&amp;controls=0&amp;showinfo=0</video>
        <url>https://profiles.stanford.edu/allison-okamura</url>
        <guest>Mark Yim</guest>
        <guesturl>https://www.grasp.upenn.edu/people/mark-yim</guesturl>
    </talk>
    <talk>
        <speaker>Anca Dragan</speaker>
        <affliation>UC Berkeley</affliation>
        <date>12 June 2020</date>
        <title>Optimizing Intended Reward Functions: Extracting all the right information from all the right places</title>
        <abstract>AI work tends to focus on how to optimize a specified reward function, but rewards that lead to the desired behavior consistently are not so easy to specify. Rather than optimizing specified reward, which is already hard, robots have the much harder job of optimizing intended reward. While the specified reward does not have as much information as we make our robots pretend, the good news is that humans constantly leak information about what the robot should optimize. In this talk, we will explore how to read the right amount of information from different types of human behavior -- and even the lack thereof. </abstract>
        <bio>Anca Dragan is an Assistant Professor in EECS at UC Berkeley, where she runs the InterACT lab. Her goal is to enable robots to work with, around, and in support of people. She works on algorithms that enable robots to a) coordinate with people in shared spaces, and b) learn what people want them to do. Anca did her PhD in the Robotics Institute at Carnegie Mellon University on legible motion planning. At Berkeley, she helped found the Berkeley AI Research Lab, is a co-PI for the Center for Human-Compatible AI, and has been honored by the Presidential Early Career Award for Scientists and Engineers (PECASE), the Sloan fellowship, the NSF CAREER award, the Okawa award, MIT's TR35, and an IJCAI Early Career Spotlight.</bio>
        <graphic>anca_dragan.jpg</graphic>
        <video>https://www.youtube.com/embed/6pKv9YUCFkw?rel0&amp;controls=0&amp;showinfo=0</video>
        <url>https://people.eecs.berkeley.edu/~anca/</url>
        <guest>Ayanna Howard</guest>
        <guesturl>https://howard.ece.gatech.edu/</guesturl>
    </talk>
    <talk>
        <speaker>Scott Kuindersma</speaker>
        <affliation>Boston Dynamics</affliation>
        <date>26 June 2020</date>
        <title>Recent Progress on Atlas, the World’s Most Dynamic Humanoid Robot</title>
        <abstract>The Atlas project at Boston Dynamics aims to make advances in robot hardware and software that allow us to match or exceed average human performance in dynamic mobility tasks. In this talk, I will give an overview of how we are using optimization to rapidly create behaviors for Atlas and share some recent examples of applying these ideas to perception-driven parkour.</abstract>
        <bio>Scott Kuindersma is a Research Scientist at Boston Dynamics where he leads a team of control engineers who are creating algorithms for coordinated, athletic, and adaptive behavior on Atlas. Previously he held positions as an Assistant Professor of Engineering and Computer Science at Harvard University and a postdoc at MIT CSAIL where he was the controls lead for the MIT DARPA Robotics Challenge Team.</bio>
        <graphic>scott_kuindersma.jpg</graphic>
        <video>https://www.youtube.com/embed/EGABAx52GKI?rel0&amp;controls=0&amp;showinfo=0</video>
        <url>https://scottk.seas.harvard.edu/</url>
        <guest>Sangbae Kim</guest>
        <guesturl>https://meche.mit.edu/people/faculty/SANGBAE@MIT.EDU</guesturl>
    </talk>
    <talk>
        <speaker>Naira Hovakimyan</speaker>
        <affliation>UIUC</affliation>
        <date>10 July 2020</date>
        <graphic>naira_hovakimyan.jpg</graphic>
        <url>https://naira.mechse.illinois.edu/sciencex_teams/naira-hovakimyan/</url>
    </talk>
    <talk>
        <speaker>Sidd Srinivasa</speaker>
        <affliation>UW</affliation>
        <date>24 July 2020</date>
        <graphic>sidd_srinivasa.jpg</graphic>
        <url>https://goodrobot.ai/</url>
    </talk>
    <!--<talk>
        <speaker>Person Name</speaker>
        <affliation>School</affliation>
        <date>25 March 2020</date>
        <title>Talk Title</title>
        <abstract>Lorem ipsum dolor sit amet, consectetur adipiscing elit. </abstract>
        <bio>Lorem ipsum dolor sit amet, consectetur adipiscing elit. </bio>
        <graphic>dummy.jpg</graphic>
        <video>https://www.youtube.com/embed/SDmbGrQqWog?rel0&amp;controls=0&amp;showinfo=0</video>
    </talk>-->  
</events>
